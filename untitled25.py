# -*- coding: utf-8 -*-
"""Untitled25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1npCJkJp1yTtjkE7ObXEkdWGwPU7WD_mj

**get the data**
"""

import pandas as pd

file_path = '/content/Brand Experience and Authenticity in Aesthetic Dental Clinics_ Effects on Customer Loyalty Survey (الردود) - ردود النموذج 1 (1).csv'
df = pd.read_csv(file_path)

print(f'Successfully loaded {len(df)} rows and {len(df.columns)} columns.')
display(df.head())

"""drop colomn('طابع زمني')"""

df = df.drop(columns=['طابع زمني'])
print("Column 'طابع زمني' dropped successfully.")
display(df.head())

"""drop duplication"""

df_cleaned = df.drop_duplicates().copy()

print(f"Original rows: {len(df)}")
print(f"Rows after removing duplicates: {len(df_cleaned)}")
print(f"Duplicates removed: {len(df) - len(df_cleaned)}")

"""translate each arabic value to english value"""

pip install googletrans==4.0.0-rc1

import pandas as pd
from googletrans import Translator,LANGUAGES
import re
import time

# Step 3: Translate Arabic names to English
translator = Translator()

def contains_arabic(text):
    """Check if text contains Arabic characters"""
    if pd.isna(text):
        return False
    arabic_pattern = re.compile('[\u0600-\u06FF]+')
    return bool(arabic_pattern.search(str(text)))

def translate_arabic_name(name):
    """Translate Arabic text to English"""
    if pd.isna(name):
        return name

    try:
        if contains_arabic(str(name)):
            result = translator.translate(str(name), src='ar', dest='en')
            time.sleep(0.3)  # Small delay to avoid rate limiting
            return result.text
        else:
            return name
    except Exception as e:
        print(f"Error translating '{name}': {e}")
        return name

# Apply translation to the Name column
print("\nTranslating Arabic names...")
df_cleaned['Name'] = df_cleaned['Name'].apply(translate_arabic_name)


# Apply translation to the 'Occupation ' column (corrected column name)
df_cleaned['Occupation '] = df_cleaned['Occupation '].apply(translate_arabic_name)

# Save the cleaned data
output_filename = 'Cleaned_Survey_Data.csv'
df_cleaned.to_csv(output_filename, index=False)

print(f"\nCleaned data saved to: {output_filename}")
print(f"\nFirst few rows of cleaned data:")
print(df_cleaned[['Name', 'Gender ', 'Age', 'Occupation ']].head(10))

# Summary statistics
print(f"\n=== Cleaning Summary ===")
print(f"Total original records: {len(df)}")
print(f"Total cleaned records: {len(df_cleaned)}")
print(f"Records removed: {len(df) - len(df_cleaned)}")
print(f"Columns: {len(df_cleaned.columns)}")

"""remove noise and unvalid values"""

df_cleaned['Occupation '] = df_cleaned['Occupation '].str.replace(r'[^a-zA-Z\s&-]', '', regex=True)

df_cleaned['Occupation '] = df_cleaned['Occupation '].str.strip()
df_cleaned['Occupation '] = df_cleaned['Occupation '].str.title()

invalid_values = ['Agree', 'M', 'L', 'Zay A', 'Yas', 'Nancy', 'Hsnia', 'Farida', 'Mba', 'Good', 'No', 'Zz', 'Grad', 'Non',
                  'Cairo', 'The Place Is Nice Clean And Comfortable', '', '.','The Service Was Good And The Clinic Was Excellent', 'Balsta', 'Dr', 'Mangoo', 'Mohamed']

df_cleaned = df_cleaned[~df_cleaned['Occupation '].isin(invalid_values)]

df_cleaned['Name'] = df_cleaned['Name'].apply(lambda x: ' '.join(str(x).split()[:2]))

df_cleaned = df_cleaned[df_cleaned['Name'].apply(lambda x: pd.notnull(x) and x != '' and all(len(p) > 1 for p in x.split()))]

df_cleaned['Occupation '] = df_cleaned['Occupation '].replace({
    'Dentist/Content Creator': 'Dentist',
    'Dentist- Content Creator': 'Dentist',
    'Graphic Design& Editor': 'Graphic Designer',
    'Dental Call Center Team Leader':'team leader',
    'I Work For Cashier System Company To Service Cashier Systems':'Service Cashier Systems'
})

print(df_cleaned['Occupation '].unique())

df_cleaned.head()

"""download cleaned_data"""

output_filename = 'cleaned_survey_data.csv'
df_cleaned.to_csv(output_filename, index=False)

print(f"Cleaned data saved to: {output_filename}")

from google.colab import files
files.download(output_filename)

"""convert data to numeric to make descriptive stastics"""

mapping = {
    'Strongly Disagree': 1,
    'Disagree': 2,
    'Neutral': 3,
    'Netural': 3,
    'Agree': 4,
    'Strongly Agree': 5,
    'Strongly agree': 5
}

likert_columns = df_cleaned.columns[4:]
for col in likert_columns:
    df_cleaned[col] = df_cleaned[col].map(mapping)

"""descriptive statistics for each question"""

desc_stats = df_cleaned[likert_columns].describe()
print(desc_stats)

"""Reliability"""

!pip install pingouin

import pingouin as pg

alpha = pg.cronbach_alpha(df_cleaned[likert_columns])
print("Cronbach's Alpha:", alpha)

"""Correlation"""

import seaborn as sns
import matplotlib.pyplot as plt

corr_matrix = df_cleaned[likert_columns].corr()
plt.figure(figsize=(15,12))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Matrix of Survey Questions")
plt.show()

"""Regression"""

from sklearn.linear_model import LinearRegression

# اختيار بعض الأسئلة كمؤشرات للمتغيرات المستقلة
# Use df_cleaned which contains the numerical mappings
X = df_cleaned[[
    'The level of professionalism in this dental practice makes me feel confident.',
    'Explaining my case diagnosis by the dentist engages me intellectually.'
]]
y = df_cleaned['I am committed to this dental practice.']

# حذف أي صفوف فيها NaN في X أو y
data_clean = pd.concat([X, y], axis=1).dropna()
X_clean = data_clean[X.columns]
y_clean = data_clean[y.name]

# تدريب نموذج الانحدار
model = LinearRegression()
model.fit(X_clean, y_clean)

print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)

"""visualizations

This code calculates how strongly each survey question is correlated with the main recommendation question and visualizes the most influential questions using a bar chart.
"""

import matplotlib.pyplot as plt

recommend_cols = [
    "I recommended this dental practice to my family and friends.",
    "I speak positively about this dental practice with others",
    "If someone mentions dental practice service, I will immediately suggest this dental practice",
    "If someone makes a negative comment about this dental practice brand, I will defend it"
]

# Redefine likert_question_cols for this cell to ensure it includes only original survey questions
# excluding the engineered dimensions, and uses df_cleaned's stripped column names.
all_cols = df_cleaned.columns.tolist()
# Corrected exclude_cols_for_likert with accurate column names including trailing spaces
exclude_cols_for_likert = ['Name', 'Gender ', 'Age', 'Occupation ', 'Sensory', 'Emotional', 'Professional']
likert_question_cols_for_corr = [col for col in all_cols if col not in exclude_cols_for_likert]

# correlation between each Likert question and the first recommendation question
corr_with_recommend = df_cleaned[likert_question_cols_for_corr].corrwith(df_cleaned[recommend_cols[0]])

# Sort by absolute impact
corr_sorted = corr_with_recommend.reindex(corr_with_recommend.abs().sort_values(ascending=False).index)

# Plot Bar Chart
plt.figure(figsize=(12,8))
bars = plt.barh(corr_sorted.index, corr_sorted.values, color='skyblue')
plt.xlabel("Correlation with Recommendation")
plt.title(f"Impact of Survey Questions on '{recommend_cols[0]}'")
plt.gca().invert_yaxis()  # Highest impact at the top

for i, v in enumerate(corr_sorted.values):
    plt.text(v + 0.01 if v>=0 else v - 0.05, i, f"{v:.2f}", va='center')

plt.show()

"""This code calculates correlations among all Likert-scale questions and visualizes them in a heatmap to show how each question relates to the commitment outcome.

"""

import seaborn as sns
import matplotlib.pyplot as plt

# The target variable y_col ('I am committed to this dental practice.') is one of the Likert questions.
y_col = 'I am committed to this dental practice.' # Ensure y_col is defined for this context

# Identify all Likert-scale questions, excluding the engineered dimensions and identifying info
# Assuming df_cleaned.columns has already been stripped and is consistent
all_cols = df_cleaned.columns.tolist()
# Corrected exclude_cols with accurate column names including trailing spaces
exclude_cols = ['Name', 'Gender ', 'Age', 'Occupation ', 'Sensory', 'Emotional', 'Professional']

# Get only the original Likert question columns (including the target variable for correlation)
likert_question_cols = [col for col in all_cols if col not in exclude_cols]

# correlation matrix for all Likert questions
all_corr = df_cleaned[likert_question_cols].corr()

plt.figure(figsize=(15,12))
sns.heatmap(all_corr, annot=False, cmap='coolwarm', center=0)
plt.title("Correlation Matrix of All Questions with Commitment")
plt.show()

""" This code visualizes the regression coefficients to show how each dimension affects commitment.

"""

dimensions = list(X_clean.columns)  # ['The level of professionalism...', 'Explaining my case diagnosis...']


import matplotlib.pyplot as plt

plt.figure(figsize=(10,5))
bars = plt.bar(dimensions, model.coef_, color=['skyblue', 'salmon'])
plt.title('Impact of Each Dimension on Commitment')
plt.ylabel('Regression Coefficient')
plt.ylim(min(model.coef_)-0.05, max(model.coef_)+0.05)

for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, f'{yval:.3f}', ha='center', va='bottom')

plt.xticks(rotation=30, ha='right')
plt.show()

"""These lists define the survey questions belonging to each dimension (Sensory, Emotional, Professional).

"""

# Sensory Experience
sensory_cols = [
    'The interior design and decorations of this dental practice appeal to my visual sense',
    'The odors of this dental practice appeal to my smell sense.',
    'I find this dental practice interesting in a sensory way'
]

# Emotional Attachment
emotional_cols = [
    'The sympathetic attitude of the staff creates a strong emotional attachment to this practice.',
    'I have positive feelings for this dental practice',
    'Considering my concerns to the dentist makes me feel secured.'
]

# Professionalism
professional_cols = [
    'The level of professionalism in this dental practice makes me feel confident.',
    'Explaining my case diagnosis by the dentist engages me intellectually.'
]

"""defines the survey questions for each dimension (Sensory, Emotional, Professional),
and computes the average score for each dimension to create new composite columns.
"""

df_cleaned.columns = df_cleaned.columns.str.strip()

# Sensory Experience
sensory_cols = [
    'The interior design and decorations of this dental practice appeal to my visual sense',
    'The odors of this dental practice appeal to my smell sense.',
    'I find this dental practice interesting in a sensory way'
]

# Emotional Attachment
emotional_cols = [
    'The sympathetic attitude of the staff creates a strong emotional attachment to this practice.',
    'I have positive feelings for this dental practice',
    'Considering my concerns to the dentist makes me feel secured.'
]

# Professionalism
professional_cols = [
    'The level of professionalism in this dental practice makes me feel confident.',
    'Explaining my case diagnosis by the dentist engages me intellectually.'
]

df_cleaned['Sensory'] = df_cleaned[sensory_cols].mean(axis=1)
df_cleaned['Emotional'] = df_cleaned[emotional_cols].mean(axis=1)
df_cleaned['Professional'] = df_cleaned[professional_cols].mean(axis=1)

df_cleaned[['Sensory','Emotional','Professional']].corr()

likert_map = {
    'Strongly Disagree': 1,
    'Disagree': 2,
    'Neutral': 3,
    'Agree': 4,
    'Strongly Agree': 5
}

all_cols = sensory_cols + emotional_cols + professional_cols
df_cleaned[all_cols] = df_cleaned[all_cols].replace(likert_map)

import numpy as np

def cronbach_alpha(df_group):
    df_corr = df_group.corr()
    N = df_group.shape[1]
    avg_r = df_corr.values[np.triu_indices(N, 1)].mean()
    alpha = (N * avg_r) / (1 + (N - 1) * avg_r)
    return alpha

groups = {
    'Sensory': sensory_cols,
    'Emotional': emotional_cols,
    'Professional': professional_cols
}

for group_name, cols in groups.items():
    alpha = cronbach_alpha(df_cleaned[cols])
    print(f"Cronbach's Alpha for {group_name}: {alpha:.2f}")

df_cleaned[sensory_cols].corr()
df_cleaned[emotional_cols].corr()
df_cleaned[professional_cols].corr()

import pandas as pd
import numpy as np

# This likert_map is largely redundant as Likert mapping was already done in cell SLEbiQihbGiV.
# Keeping it defined for clarity, but the mapping line will be removed.
likert_map = {
    'Strongly Disagree': 1,
    'Disagree': 2,
    'Neutral': 3,
    'Agree': 4,
    'Strongly Agree': 5
}

# Original columns for each dimension
sensory_cols = [
    'The interior design and decorations of this dental practice appeal to my visual sense',
    'The odors of this dental practice appeal to my smell sense.',
    'I find this dental practice interesting in a sensory way'
]

emotional_cols = [
    'The sympathetic attitude of the staff creates a strong emotional attachment to this practice.',
    'I have positive feelings for this dental practice',
    'Considering my concerns to the dentist makes me feel secured.'
]

professional_cols = [
    'The level of professionalism in this dental practice makes me feel confident.',
    'Explaining my case diagnosis by the dentist engages me intellectually.'
]

# The 'new_professional_cols' are not present in df_cleaned and cannot be analyzed.
# If you wish to include them, you would need to add this data to your DataFrame first.
# For now, we will proceed with the existing 'professional_cols'.
# new_professional_cols = [
#     'The dentist clearly explains the treatment procedure to me.',
#     'The dentist values my time and ensures I am comfortable during the treatment.',
#     'The staff responds professionally to my questions and concerns.'
# ]

# Use only the existing professional_cols for the analysis.
all_professional_cols = professional_cols

# The step to convert texts to numbers is redundant as it was already done in cell SLEbiQihbGiV.
# all_cols = sensory_cols + emotional_cols + all_professional_cols
# df_cleaned[all_cols] = df_cleaned[all_cols].replace(likert_map)

# Function to calculate Cronbach's Alpha
def cronbach_alpha(df_group):
    N = df_group.shape[1]
    # Drop rows with NaN values for a clean correlation calculation
    df_group_clean = df_group.dropna()
    if df_group_clean.empty or N < 2: # Ensure enough data and items for calculation
        return np.nan
    df_corr = df_group_clean.corr()
    # Handle cases where correlation cannot be computed (e.g., all values are same for a column)
    if df_corr.isnull().values.any():
        return np.nan
    avg_r = df_corr.values[np.triu_indices(N, 1)].mean()
    if N == 1: # Cronbach's Alpha is not meaningful for a single item
        return np.nan
    alpha = (N * avg_r) / (1 + (N - 1) * avg_r)
    return alpha

# Function to calculate Item-Total Correlation
def item_total_corr(df_group):
    results = {}
    df_group_clean = df_group.dropna() # Drop NaN rows for consistent calculation
    if df_group_clean.empty:
        return {col: np.nan for col in df_group.columns}
    for col in df_group_clean.columns:
        other_cols = [c for c in df_group_clean.columns if c != col]
        if not other_cols: # Handle case where there's only one column
             results[col] = np.nan
             continue
        # Calculate correlation between item and sum of other items
        corr = df_group_clean[col].corr(df_group_clean[other_cols].sum(axis=1))
        results[col] = corr
    return results

# Calculate Cronbach Alpha for each dimension
groups = {
    'Sensory': sensory_cols,
    'Emotional': emotional_cols,
    'Professional': all_professional_cols # Using the corrected list
}

for group_name, cols in groups.items():
    # Ensure there are at least 2 columns to calculate Cronbach's alpha and correlation
    if len(cols) < 2:
        print(f"Skipping Cronbach's Alpha for {group_name}: Not enough items (need at least 2).")
        print("-" * 50)
        continue

    # Filter df_cleaned to only include the relevant columns for the current group
    df_group_data = df_cleaned[cols]

    alpha = cronbach_alpha(df_group_data)
    print(f"Cronbach's Alpha for {group_name}: {alpha:.2f}")
    # Item-Total Correlation
    corrs = item_total_corr(df_group_data)
    print(f"Item-Total Correlations for {group_name}:")
    for q, val in corrs.items():
        print(f"  {q[:50]}...: {val:.2f}")  # Limit display to first 50 characters
    print("-"*50)

"""This plots histograms for the Sensory, Emotional, and Professional dimensions to show the distribution of their scores across respondents.

"""

df_cleaned[['Sensory', 'Emotional', 'Professional']].hist(bins=5, figsize=(10,4), color='skyblue', edgecolor='black')
plt.suptitle("Histogram of Each Dimension")
plt.show()

"""a boxplot for the Sensory, Emotional, and Professional dimensions to visualize their distributions and identify potential outliers.

"""

plt.figure(figsize=(8,6))
sns.boxplot(data=df_cleaned[['Sensory', 'Emotional', 'Professional']])
plt.title("Distribution of Each Dimension")
plt.ylabel("Likert Scale Value")
plt.show()

"""scatter plots of each dimension (Sensory, Emotional, Professional) against the commitment variable to visualize their relationships.

"""

sns.pairplot(df_cleaned, x_vars=['Sensory','Emotional','Professional'], y_vars=y_col, height=4, kind='scatter')
plt.show()

"""the correlation of each survey question with the commitment variable and visualizes the results as a horizontal bar chart to highlight the most influential questions."""

import matplotlib.pyplot as plt

# The target variable y_col ('I am committed to this dental practice.') is one of the Likert questions.
y_col = 'I am committed to this dental practice.'

# Identify all Likert-scale questions, excluding the engineered dimensions and identifying info
# Assuming df_cleaned.columns has already been stripped and is consistent
all_cols = df_cleaned.columns.tolist()
exclude_cols = ['Name', 'Gender', 'Age', 'Occupation', 'Sensory', 'Emotional', 'Professional']

# Get only the original Likert question columns (including the target variable for correlation)
likert_question_cols = [col for col in all_cols if col not in exclude_cols]

corr_with_commitment = df_cleaned[likert_question_cols].corrwith(df_cleaned[y_col])

corr_sorted = corr_with_commitment.reindex(corr_with_commitment.abs().sort_values(ascending=False).index)


plt.figure(figsize=(12,8))
bars = plt.barh(corr_sorted.index, corr_sorted.values, color='skyblue')
plt.xlabel("Correlation with Commitment")
plt.title("Impact of Survey Questions on Commitment")
plt.gca().invert_yaxis()

for i, v in enumerate(corr_sorted.values):
    plt.text(v + 0.01 if v>=0 else v - 0.05, i, f"{v:.2f}", va='center')

plt.show()

"""scatter plots for the top 5 questions most correlated with commitment to visually examine their relationships with the commitment variable."""

import seaborn as sns
top5_questions = corr_sorted.head(5).index.tolist()
sns.pairplot(df, x_vars=top5_questions, y_vars=y_col, height=7)
plt.show()

# Commitment column
y_col = 'I am committed to this dental practice.'

# Average commitment per gender
gender_effect = df_cleaned.groupby('Gender ')[y_col].mean()

print(gender_effect)

import matplotlib.pyplot as plt

gender_effect.plot(kind='bar', color=['skyblue','salmon'])
plt.title("Commitment Level by Gender")
plt.ylabel("Average Commitment Score")
plt.xlabel("Gender")
plt.ylim(0,5)
plt.show()

# Convert 'Age' column to numeric (e.g., take the lower bound of the range)
df_cleaned['Age_Numeric'] = df_cleaned['Age'].astype(str).str.extract(r'(\d+)').astype(int)

# Create age bins
bins = [18, 25, 35, 45, 60]
labels = ['18-25', '26-35', '36-45', '46-60']

df_cleaned['Age_Range'] = pd.cut(df_cleaned['Age_Numeric'], bins=bins, labels=labels, include_lowest=True)

# Average commitment per age range
age_effect = df_cleaned.groupby('Age_Range', observed=False)[y_col].mean()

print(age_effect)

age_effect.plot(kind='bar', color='lightgreen')
plt.title("Commitment Level by Age Group")
plt.ylabel("Average Commitment Score")
plt.xlabel("Age Group")
plt.ylim(0,5)
plt.show()

rec_col = "I recommended this dental practice to my family and friends."

df_cleaned.groupby("Gender ")[rec_col].mean()

df_cleaned.groupby("Age_Range")[rec_col].mean()

import pandas as pd
import numpy as np

# ----- تحويل نصوص Likert لأرقام ----- (This step is redundant as it was done previously)
# likert_map = {
#     'Strongly Disagree': 1,
#     'Disagree': 2,
#     'Neutral': 3,
#     'Agree': 4,
#     'Strongly Agree': 5
# }

# الأعمدة الأصلية لكل محور
sensory_cols = [
    'The interior design and decorations of this dental practice appeal to my visual sense',
    'The odors of this dental practice appeal to my smell sense.',
    'I find this dental practice interesting in a sensory way'
]

emotional_cols = [
    'The sympathetic attitude of the staff creates a strong emotional attachment to this practice.',
    'I have positive feelings for this dental practice',
    'Considering my concerns to the dentist makes me feel secured.'
]

professional_cols = [
    'The level of professionalism in this dental practice makes me feel confident.',
    'Explaining my case diagnosis by the dentist engages me intellectually.'
]

# ----- أسئلة جديدة لمحور Professionalism (These columns do not exist in the DataFrame) -----
# new_professional_cols = [
#     'The dentist clearly explains the treatment procedure to me.',
#     'The dentist values my time and ensures I am comfortable during the treatment.',
#     'The staff responds professionally to my questions and concerns.'
# ]

# دمج الأسئلة الجديدة مع القديمة - Using only existing professional_cols
all_professional_cols = professional_cols

# ----- تحويل النصوص لأرقام ----- (This step is redundant as it was done previously)
# all_cols = sensory_cols + emotional_cols + all_professional_cols
# df_cleaned[all_cols] = df_cleaned[all_cols].replace(likert_map)

# ----- دالة لحساب Cronbach's Alpha -----
def cronbach_alpha(df_group):
    N = df_group.shape[1]
    df_corr = df_group.corr()
    avg_r = df_corr.values[np.triu_indices(N, 1)].mean()
    alpha = (N * avg_r) / (1 + (N - 1) * avg_r)
    return alpha

# ----- دالة لحساب Item-Total Correlation -----
def item_total_corr(df_group):
    results = {}
    for col in df_group.columns:
        other_cols = [c for c in df_group.columns if c != col]
        corr = df_group[col].corr(df_group[other_cols].mean(axis=1))
        results[col] = corr
    return results

# ----- حساب Cronbach Alpha لكل محور -----
groups = {
    'Sensory': sensory_cols,
    'Emotional': emotional_cols,
    'Professional': all_professional_cols
}

for group_name, cols in groups.items():
    alpha = cronbach_alpha(df_cleaned[cols])
    print(f"Cronbach's Alpha for {group_name}: {alpha:.2f}")

    # Item-Total Correlation لكل سؤال
    corrs = item_total_corr(df_cleaned[cols])
    print(f"Item-Total Correlations for {group_name}:")
    for q, val in corrs.items():
        print(f"  {q[:50]}...: {val:.2f}")

    # حساب متوسط كل محور لكل مشارك (جاهز للـ KPI)
    df_cleaned[group_name+'_Avg'] = df_cleaned[cols].mean(axis=1)
    print(f"Average values for {group_name} saved in column '{group_name}_Avg'\n")

from sklearn.linear_model import LinearRegression

# Independent variables (X) using the average scores for each dimension
X = df_cleaned[['Sensory', 'Emotional', 'Professional']]

# Dependent variable (y) remains 'I am committed to this dental practice.'
y = df_cleaned['I am committed to this dental practice.']

# Drop rows with NaN values in either X or y
data_for_regression = pd.concat([X, y], axis=1).dropna()
X_reg = data_for_regression[['Sensory', 'Emotional', 'Professional']]
y_reg = data_for_regression['I am committed to this dental practice.']

# Create and train the Linear Regression model
model_dimensions = LinearRegression()
model_dimensions.fit(X_reg, y_reg)

print("Regression Coefficients for Dimensions:")
for dim, coef in zip(X_reg.columns, model_dimensions.coef_):
    print(f"{dim}: {coef:.3f}")
print(f"\nIntercept: {model_dimensions.intercept_:.3f}")



from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Make predictions using the trained model
y_pred = model_dimensions.predict(X_reg)

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(y_reg, y_pred)

# Calculate R-squared (R2) score
r2 = r2_score(y_reg, y_pred)

print(f"Mean Squared Error (MSE): {mse:.3f}")
print(f"R-squared (R2) Score: {r2:.3f}")

"""# Task
Perform a linear regression analysis using a new set of individual survey questions as independent variables to predict 'I am committed to this dental practice.' as the dependent variable. After training the model, calculate and display the regression coefficients, intercept, Mean Squared Error (MSE), and R-squared (R2) to evaluate its performance.

## select_new_features

### Subtask:
Select a new set of individual survey questions as independent variables for the regression model.

**Reasoning**:
I need to define the new independent variables and the dependent variable as specified in the instructions for the regression model. This involves creating a list for the independent variables and assigning the dependent variable as a string.
"""

new_independent_cols = [
    'I trust this dental practice.',
    'I get a special feeling from this dental practice.',
    'The modern equipment used by this dental practice stimulates my curiosity',
    'I recommended this dental practice to my family and friends.'
]

y_col = 'I am committed to this dental practice.'

print("New independent variables defined.")
print(f"Dependent variable defined as: {y_col}")

"""**Reasoning**:
I need to prepare the independent and dependent variables for the linear regression model by selecting the specified columns from the cleaned DataFrame and handling any missing values.


"""

new_independent_cols = [
    'The level of professionalism in this dental practice makes me feel confident.',
    'I have positive feelings for this dental practice',
    'The modern equipment used by this dental practice stimulates my curiosity',
    'I recommended this dental practice to my family and friends.'
]

y_col = 'I am committed to this dental practice.'

print("New independent variables defined.")
print(f"Dependent variable defined as: {y_col}")

new_independent_cols = [
    'The level of professionalism in this dental practice makes me feel confident.',
    'I have positive feelings for this dental practice',
    'The modern equipment used by this dental practice stimulates my curiosity',
    'I recommended this dental practice to my family and friends.'
]

y_col = 'I am committed to this dental practice.'

X_new = df_cleaned[new_independent_cols]
y_new = df_cleaned[y_col]

data_for_new_regression = pd.concat([X_new, y_new], axis=1).dropna()
X_reg_new = data_for_new_regression[new_independent_cols]
y_reg_new = data_for_new_regression[y_col]

print(f"Prepared {len(X_reg_new)} rows for new regression analysis.")

from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

y_pred_new = model_new_features.predict(X_reg_new)

mse_new = mean_squared_error(y_reg_new, y_pred_new)
r2_new = r2_score(y_reg_new, y_pred_new)

print(f"Mean Squared Error (MSE) for new model: {mse_new:.3f}")
print(f"R-squared (R2) Score for new model: {r2_new:.3f}")